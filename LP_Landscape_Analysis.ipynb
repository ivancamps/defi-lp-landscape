{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkSrPyGN0030"
      },
      "source": [
        "# DeFi LP Landscape Update"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uefzj5QS1LGk"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyOTepQJ1hFX"
      },
      "source": [
        "### Install and import all relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "KsWIyhkz6UtD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gspread\n",
        "!pip install pandas\n",
        "!pip install gspread-dataframe\n",
        "!pip install oauth2client\n",
        "!pip install PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "T0GACseh6dcV"
      },
      "outputs": [],
      "source": [
        "# Import all relevant libraries\n",
        "import pandas as pd\n",
        "import json\n",
        "import time\n",
        "from datetime import datetime as dt\n",
        "import csv\n",
        "import requests\n",
        "import gspread\n",
        "import gspread_dataframe as gd\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry-Z_lO613tC"
      },
      "source": [
        "### Connect to Google Sheets API and setup Defillama API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1vV2-rvL8JX",
        "outputId": "b1747c9e-88ad-4989-bf1d-bce4d0594a70"
      },
      "outputs": [],
      "source": [
        "# Set up Google Sheets API credentials\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "credentials = ServiceAccountCredentials.from_json_keyfile_name('lp-landscape-analysis-dd6d6479b244.json', scope)\n",
        "gc = gspread.authorize(credentials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "UEcUo3T06ghU"
      },
      "outputs": [],
      "source": [
        "# API TVL base URL\n",
        "tvl_base_url = 'https://api.llama.fi'\n",
        "\n",
        "# API Yields base URL\n",
        "yields_base_url = 'https://yields.llama.fi'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbrFKqGO2Liv"
      },
      "source": [
        "## 2. Update Data in 'DeFi Landscape LP Opportunities'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBIh0TWv245K"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "ZderTHLInaNn"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "# Writes a list of values to a column of the input sheet starting at starting_cell\n",
        "def write_to_column(sheet, starting_cell, values):\n",
        "  sheet.update(starting_cell, [[i] for i in values], value_input_option=\"USER_ENTERED\")\n",
        "  print(\"Values written to sheet successfully\")\n",
        "\n",
        "# Converts a dataframe column from str to date format in-place\n",
        "def df_str_to_date(df, column_name, date_format):\n",
        "  df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='raise')\n",
        "  df[column_name] = df[column_name].dt.date\n",
        "\n",
        "# Write list to CSV\n",
        "def write_list_to_csv(filename, list):\n",
        "   # Open the file in write mode\n",
        "  with open(filename, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    \n",
        "    # Writing each item in the list to the file\n",
        "    for element in list:\n",
        "        writer.writerow([element])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YLemCeB3B6g"
      },
      "source": [
        "### Open spreadsheet and load relevant tabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "B9qHVp9pPKBN"
      },
      "outputs": [],
      "source": [
        "# Open the Google Sheet we'll be reading and writing to\n",
        "lp_landscape = gc.open('DeFi Landscape LP Opportunities_v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "tkFKixHVjeId"
      },
      "outputs": [],
      "source": [
        "# Select the 'Project Ratings' tab and read the data into a dataframe\n",
        "project_ratings_sheet = lp_landscape.worksheet(\"Project Ratings\")\n",
        "lp_project_ratings = project_ratings_sheet.get_all_records()\n",
        "lp_project_ratings_df = pd.DataFrame(lp_project_ratings)\n",
        "\n",
        "# Select the 'Stables' tab and read the data into a dataframe\n",
        "stables_sheet = lp_landscape.worksheet(\"Stables\")\n",
        "lp_stables = stables_sheet.get_all_records()\n",
        "lp_stables_df = pd.DataFrame(lp_stables)\n",
        "\n",
        "# Select the 'Volatile' tab and read the data into a dataframe\n",
        "volatile_sheet = lp_landscape.worksheet(\"Volatile\")\n",
        "lp_volatile = volatile_sheet.get_all_records()\n",
        "lp_volatile_df = pd.DataFrame(lp_volatile)\n",
        "\n",
        "# Select 'Pool Yields' tab\n",
        "pool_yields_sheet = lp_landscape.worksheet(\"Pool Yields\")\n",
        "\n",
        "# Select 'Protocol Historicals' tab\n",
        "protocol_historicals_sheet = lp_landscape.worksheet(\"Protocol Historicals\")\n",
        "\n",
        "# Select 'Pool Historicals' tab\n",
        "pool_historicals_sheet = lp_landscape.worksheet(\"Pool Historicals\")\n",
        "\n",
        "# Select 'Protocol Info' tab\n",
        "protocol_info_sheet = lp_landscape.worksheet(\"Protocol Info\")\n",
        "\n",
        "# Select 'Historical Chain TVL' tab\n",
        "historical_chain_tvl_sheet = lp_landscape.worksheet(\"Historical Chain TVL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uym7Wwq_3GnL"
      },
      "source": [
        "### 2.1. Update data in 'Project Ratings' tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "u6iuvMM3p3fw"
      },
      "outputs": [],
      "source": [
        "# Extract protocol names from the worksheet as a list in the format used by Defillama API (slug)\n",
        "protocol_slugs = lp_project_ratings_df[\"API Protocol Name\"].tolist()\n",
        "protocol_slugs = [protocol for protocol in protocol_slugs if len(protocol) > 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write list to csv file\n",
        "write_list_to_csv(\"protocol_slugs.csv\", protocol_slugs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvSe4aLopETQ"
      },
      "source": [
        "#### 2.1.1. Update current protocol TVLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "XBYJW-jYXYAb"
      },
      "outputs": [],
      "source": [
        "# Get current TVLs for the list of protocols and return them as a list\n",
        "protocol_tvls = [requests.get(tvl_base_url + '/tvl/' + protocol).json() for protocol in protocol_slugs]\n",
        "\n",
        "# Format numbers as millions before writing to spreadsheet\n",
        "protocol_tvls_m = [int(tvl) / 1000000 for tvl in protocol_tvls]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL6khy_lqvU3",
        "outputId": "c9ed1fb9-d746-4c65-9f31-c445b37616a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vf/7r30jcsd0190g5bqdf7ryl800000gn/T/ipykernel_82881/640623663.py:5: DeprecationWarning: [Deprecated][in version 6.0.0]: Method signature's arguments 'range_name' and 'values' will change their order. We recommend using named arguments for minimal impact. In addition, the argument 'values' will be mandatory of type: 'List[List]'. (ex) Worksheet.update(values = [[]], range_name=) \n",
            "  sheet.update(starting_cell, [[i] for i in values], value_input_option=\"USER_ENTERED\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Values written to sheet successfully\n"
          ]
        }
      ],
      "source": [
        "# Write results stored in protocol_tvls to 'Current TVL ($m)' in 'Project Ratings' tab to update current protocol TVLs\n",
        "write_to_column(project_ratings_sheet, \"C2\", protocol_tvls_m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_oYs6sfpW1K"
      },
      "source": [
        "#### 2.1.2. Update 1-year average protocol TVLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "fEPRILvz6LZI"
      },
      "outputs": [],
      "source": [
        "# Pull historical TVL data for each protocol in the list broken down by token and chain\n",
        "protocol_historicals = [requests.get(tvl_base_url + '/protocol/' + protocol).json() for protocol in protocol_slugs]\n",
        "\n",
        "# List of keys we want to keep\n",
        "fields = ['name','currentChainTvls']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNOEFuiYpd3A"
      },
      "source": [
        "##### Sanity check to compare aggregated TVL to actual current TVL shown by Defillama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "SaW0zUqBr5D0"
      },
      "outputs": [],
      "source": [
        "# Create a list of dictionaries with protocol name as the key as current TVLs\n",
        "# by chain as the value for every protocol in the list\n",
        "\n",
        "# List to store broken down results\n",
        "protocol_tvls_by_chain = []\n",
        "\n",
        "# List to store aggreagated TVL results\n",
        "aggregated_protocol_tvls = []\n",
        "\n",
        "# Iterate protocol list\n",
        "for protocol in protocol_historicals:\n",
        "  # keep only relevant keys\n",
        "  p = dict((k, protocol[k]) for k in fields if k in protocol)\n",
        "\n",
        "  # Create key,value pair from the values associated with the two keys left\n",
        "  p = {protocol['name']: protocol['currentChainTvls']}\n",
        "\n",
        "  # Iterate nested dict and remove unwanted chains to prevent double counting TVL\n",
        "  for k,v in p.items():\n",
        "    for x in list(v.keys()):\n",
        "      if \"borrowed\" in x or \"staking\" in x or 'pool2' in x:\n",
        "        del v[x]\n",
        "    aggregated = (k, sum(v.values()))\n",
        "\n",
        "  # Add protocol to result list\n",
        "  protocol_tvls_by_chain.append(p)\n",
        "\n",
        "  # Add protocol overall current TVL to aggregated list\n",
        "  aggregated_protocol_tvls.append(aggregated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "Qx8dAXGepcCG"
      },
      "outputs": [],
      "source": [
        "# COMPARE AGGREGATED VS. ACTUAL TVLs\n",
        "\n",
        "current_aggregated_protocol_tvls = [x[1] for x in aggregated_protocol_tvls]\n",
        "protocol_names = [x[0] for x in aggregated_protocol_tvls]\n",
        "\n",
        "# Create tuples for each protocol comparing aggregated TVL to TVL shown by DL\n",
        "protocol_tvl_deltas = list(zip(protocol_names, zip(current_aggregated_protocol_tvls, protocol_tvls)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iunEhYG6xKo",
        "outputId": "70e311fc-455e-46d9-dd6a-4db813ab4648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('AAVE', (8399486847.121011, 8653136261.547512)),\n",
              " ('Curve Finance', (2333223915.0743003, 2384855534.388074)),\n",
              " ('Compound V2', (1034400391.4292, 1040299204.7071826)),\n",
              " ('Compound V3', (1421582035.67243, 1431532987.834271)),\n",
              " ('Convex Finance', (1743387706.93097, 1736743399.9388666)),\n",
              " ('Reserve', (34083870.45423, 34130717.33080939)),\n",
              " ('MakerDAO', (6396112284.38318, 6392081481.197919)),\n",
              " ('Spark', (3159513792.12362, 3197883161.030174)),\n",
              " ('Yearn Finance', (341423602.21011, 343185426.4828875)),\n",
              " ('Frax Finance', (1201588509.03096, 1249201605.864286)),\n",
              " ('Goldfinch', (4123269.41291, 4138497.7044126242)),\n",
              " ('Balancer', (1105351481.14655, 1106200278.458088)),\n",
              " ('Flux Finance', (20895651.1826, 20901091.24959967)),\n",
              " ('Summer.fi', (3196150654.50773, 3196150654.5077276)),\n",
              " ('Stargate', (303236803.87854004, 304258600.30701023)),\n",
              " ('Aura', (576380631.27292, 580698087.7371308)),\n",
              " ('Abracadabra', (143203155.63134998, 144516042.36201513)),\n",
              " ('Velodrome', (124683270.46726, 129771333.46004833)),\n",
              " ('Idle', (36762066.72474, 36671159.3609247)),\n",
              " ('Gains Network', (42989834.493209995, 42891255.29944292)),\n",
              " ('Morpho', (856191257.1012601, 857473542.0876939)),\n",
              " ('Notional', (32704417.81125, 33551814.57302005)),\n",
              " ('Extra Finance', (30940260.0907, 30944558.19475335)),\n",
              " ('Tangible', (44073529.61744001, 43741315.54570083)),\n",
              " ('IPOR', (8593435.01619, 8649137.84562672)),\n",
              " ('Across', (106284135.86842, 106741418.4366121)),\n",
              " ('Inverse Finance', (59049380.18315, 60862508.624735594)),\n",
              " ('Synthetix', (668940238.55971, 672718257.4389656)),\n",
              " ('UwU Lend', (40240011.17739, 40410727.64153307)),\n",
              " ('Origin Dollar', (13406498.93553, 13389077.419863638)),\n",
              " ('Prisma Finance', (311454279.9299, 313330813.6585823))]"
            ]
          },
          "execution_count": 62,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "protocol_tvl_deltas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXxv__mz6VEW"
      },
      "source": [
        "##### Aggregate historical TVL broken down by token and chain to get overall historical protocol TVLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "SADoTi_z6tyj"
      },
      "outputs": [],
      "source": [
        "# Create a list of dictionaries with protocol name as the key and historical TVL\n",
        "# by token and chain as the value for every protocol in the list\n",
        "\n",
        "# List to store broken down results\n",
        "clean_protocol_historicals = []\n",
        "\n",
        "# Iterate protocol list\n",
        "for protocol in protocol_historicals:\n",
        "  # Keep only relevant keys\n",
        "  p = dict((k, protocol[k]) for k in fields if k in protocol)\n",
        "\n",
        "  # Create key,value pair from the values associated with the two keys left\n",
        "  p = {protocol['name']: protocol['chainTvls']}\n",
        "\n",
        "  # Iterate nested dict and remove unwanted chains to prevent double counting TVL\n",
        "  for k,v in p.items():\n",
        "    for x in list(v.keys()):\n",
        "      if \"borrowed\" in x or \"staking\" in x or 'pool2' in x:\n",
        "        del v[x]\n",
        "\n",
        "  # Add protocol to result list\n",
        "  clean_protocol_historicals.append(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "icBD3MLIpJtN"
      },
      "outputs": [],
      "source": [
        "# List to store the final result\n",
        "historical_protocol_tvls = []\n",
        "\n",
        "# Aggregate TVL data per date for each chain\n",
        "for protocol in clean_protocol_historicals:\n",
        "\n",
        "  # Dictionary to store result\n",
        "  aggregated_historicals = defaultdict(float)\n",
        "\n",
        "  for p, d1 in protocol.items():\n",
        "    for chain, chain_data in d1.items():\n",
        "        for entry in chain_data[\"tvl\"]:\n",
        "            date = entry[\"date\"]\n",
        "            totalLiquidityUSD = float(entry[\"totalLiquidityUSD\"])\n",
        "            aggregated_historicals[date] += totalLiquidityUSD\n",
        "\n",
        "  # Convert the aggregated data to a sorted list of tuples\n",
        "  sorted_aggregated_historicals = sorted(aggregated_historicals.items(), key=lambda x: x[0])\n",
        "\n",
        "  # Convert the aggregated data to a dataframe\n",
        "  aggregated_historicals_df = pd.DataFrame(sorted_aggregated_historicals, columns=['date', 'totalLiquidityUSD'])\n",
        "\n",
        "  # Convert the 'Date' column from UNIX timestamp to datetime format\n",
        "  aggregated_historicals_df['date'] = pd.to_datetime(aggregated_historicals_df['date'], unit='s')\n",
        "  aggregated_historicals_df['date'] = aggregated_historicals_df['date'].dt.date\n",
        "\n",
        "  # Add dataframe to result list\n",
        "  historical_protocol_tvls.append(aggregated_historicals_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "AlSLIFZ47LoK"
      },
      "outputs": [],
      "source": [
        "# Order by date in reverse chronological order\n",
        "historical_protocol_tvls = [df.sort_values(by='date', ascending=False) for df in historical_protocol_tvls]\n",
        "\n",
        "# Create a common index for all dataframes to conserve the order when concatenating\n",
        "for df in historical_protocol_tvls:\n",
        "    df.index = range(len(df))\n",
        "\n",
        "# Concatenate every dataframe in the list into one dataframe\n",
        "protocol_historicals_df = pd.concat(historical_protocol_tvls, axis = 1, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "y-Ysu1OQ5m2y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vf/7r30jcsd0190g5bqdf7ryl800000gn/T/ipykernel_82881/2339619863.py:13: DeprecationWarning: [Deprecated][in version 6.0.0]: Method signature's arguments 'range_name' and 'values' will change their order. We recommend using named arguments for minimal impact. In addition, the argument 'values' will be mandatory of type: 'List[List]'. (ex) Worksheet.update(values = [[]], range_name=) \n",
            "  protocol_historicals_sheet.update(\"A1\", [formatted_protocol_slugs])\n"
          ]
        }
      ],
      "source": [
        "# Insert 1 blank cell between each protocol slug in the list for spreadsheet formatting\n",
        "formatted_protocol_slugs = []\n",
        "\n",
        "for slug in protocol_slugs:\n",
        "  formatted_protocol_slugs.append(slug)\n",
        "  formatted_protocol_slugs.append(\"\")\n",
        "formatted_protocol_slugs.pop()\n",
        "\n",
        "# Clear every cell in the sheet\n",
        "protocol_historicals_sheet.clear()\n",
        "\n",
        "# Write formatted list as first row of the 'Protocol Historicals' sheet for indexing\n",
        "protocol_historicals_sheet.update(\"A1\", [formatted_protocol_slugs])\n",
        "\n",
        "# Update data in 'Protocol Historicals' sheet under first row\n",
        "gd.set_with_dataframe(protocol_historicals_sheet, protocol_historicals_df, row=2,\n",
        "                      include_index=False, include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O5P-vLcu0ow"
      },
      "source": [
        "### 2.2 Update 'Pool Yields' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "k87k0oPTt7FU"
      },
      "outputs": [],
      "source": [
        "# Get all pool yields\n",
        "yields = requests.get(yields_base_url + '/pools')\n",
        "\n",
        "# Convert Yields response to data frame\n",
        "pool_yields_df = pd.DataFrame(yields.json()['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "zAwTLqMuVr6W"
      },
      "outputs": [],
      "source": [
        "# Clear every cell in the sheet\n",
        "pool_yields_sheet.clear()\n",
        "\n",
        "# Update data in 'Pool Yields' sheet\n",
        "gd.set_with_dataframe(pool_yields_sheet, pool_yields_df, include_index=False,\n",
        "                      include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2hBh1Q3sN7"
      },
      "source": [
        "### 2.3 Update 'Pool Historicals' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "9JGcvfoE36Rm"
      },
      "outputs": [],
      "source": [
        "# Extract pool ID's from stables worksheet as a list\n",
        "stable_pool_ids = lp_stables_df[\"API pool id\"].tolist()\n",
        "stable_pool_ids = [id for id in stable_pool_ids if id]\n",
        "\n",
        "# Extract pool ID's from volatile worksheet as a list\n",
        "volatile_pool_ids = lp_volatile_df[\"API pool id\"].tolist()\n",
        "volatile_pool_ids = [id for id in volatile_pool_ids if id]\n",
        "\n",
        "# Join both lists\n",
        "pool_ids = stable_pool_ids + volatile_pool_ids"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write lists to csv files\n",
        "write_list_to_csv(\"stable_pool_ids.csv\", stable_pool_ids)\n",
        "write_list_to_csv(\"volatile_pool_ids.csv\", volatile_pool_ids)\n",
        "write_list_to_csv(\"pool_ids.csv\", pool_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get historical TVL and APY data for each pool and return it as a list of data frames\n",
        "\n",
        "pool_historicals = []\n",
        "counter = 0\n",
        "\n",
        "for id in pool_ids:\n",
        "    # Keep a counter to prevent API rate limit\n",
        "    counter += 1\n",
        "\n",
        "    if counter > 30:\n",
        "        counter = 0\n",
        "        # ~1 minute delay to avoid API rate limit issues\n",
        "        time.sleep(70)\n",
        "\n",
        "    try:  \n",
        "        curr = requests.get(yields_base_url + '/chart/' + id).json()['data']\n",
        "        pool_historicals.append(curr)\n",
        "    except: \n",
        "        print(\"Error parsing \" + str(id))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {},
      "outputs": [],
      "source": [
        "pool_historicals_dfs = [pd.DataFrame(i).drop(['il7d', 'apyBase7d','apyBase','apyReward'], axis=1) for i in pool_historicals]\n",
        "\n",
        "# Convert timestamp format from str to date in every data frame\n",
        "date_format = '%Y-%m-%dT%H:%M:%S.%fZ'\n",
        "\n",
        "for df in pool_historicals_dfs:\n",
        "  df_str_to_date(df, 'timestamp', date_format)\n",
        "\n",
        "# Order by date in reverse chronological order\n",
        "pool_historicals_dfs = [df.sort_values(by='timestamp', ascending=False) for df in pool_historicals_dfs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a1flnwVrW2Ht",
        "outputId": "c8195a0e-c109-462e-f47c-4fa6c9a5ebb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a common index for all dataframes to conserve the order when concatenating\n",
        "for df in pool_historicals_dfs:\n",
        "    df.index = range(len(df))\n",
        "\n",
        "# Concatenate every data frame in the list into one data frame\n",
        "pool_historicals_df = pd.concat(pool_historicals_dfs, axis = 1, sort=False)\n",
        "\n",
        "# Insert 4 blank elements between each pool id in the list for spreadsheet formatting\n",
        "formatted_pool_ids = []\n",
        "\n",
        "for id in pool_ids:\n",
        "  formatted_pool_ids.append(id)\n",
        "  for i in range(2):\n",
        "    formatted_pool_ids.append(\"\")\n",
        "formatted_pool_ids.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "x0jbl4PBVqHv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vf/7r30jcsd0190g5bqdf7ryl800000gn/T/ipykernel_82881/4049637518.py:5: DeprecationWarning: [Deprecated][in version 6.0.0]: Method signature's arguments 'range_name' and 'values' will change their order. We recommend using named arguments for minimal impact. In addition, the argument 'values' will be mandatory of type: 'List[List]'. (ex) Worksheet.update(values = [[]], range_name=) \n",
            "  pool_historicals_sheet.update(\"A1\", [formatted_pool_ids])\n"
          ]
        }
      ],
      "source": [
        "# Clear every cell in the sheet\n",
        "pool_historicals_sheet.clear()\n",
        "\n",
        "# Write formatted list as first row of the 'Pool Historicals' sheet for indexing\n",
        "pool_historicals_sheet.update(\"A1\", [formatted_pool_ids])\n",
        "\n",
        "# Update data in 'Pool Historicals' sheet under first row\n",
        "gd.set_with_dataframe(pool_historicals_sheet, pool_historicals_df, row=2,\n",
        "                      include_index=False, include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9w44YUyOdTD"
      },
      "source": [
        "### 2.4. Update 'Historical Chain TVL' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "ggNYIj2yOoTQ"
      },
      "outputs": [],
      "source": [
        "# Get historical TVL data for Ethereum\n",
        "eth_tvl = requests.get(tvl_base_url + '/v2/historicalChainTvl/Ethereum')\n",
        "\n",
        "# Convert response to data frame\n",
        "eth_tvl_df = pd.DataFrame(eth_tvl.json())\n",
        "\n",
        "# Convert the 'Date' column from UNIX timestamp to datetime format\n",
        "eth_tvl_df['date'] = pd.to_datetime(eth_tvl_df['date'], unit='s')\n",
        "eth_tvl_df['date'] = eth_tvl_df['date'].dt.date\n",
        "\n",
        "# Order by date in reverse chronological order\n",
        "eth_tvl_df = eth_tvl_df.sort_values(by='date', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "xyTTtSk8PJlV"
      },
      "outputs": [],
      "source": [
        "# Clear every cell in the sheet\n",
        "historical_chain_tvl_sheet.clear()\n",
        "\n",
        "# Update data in 'Historical Chain TVL' sheet\n",
        "gd.set_with_dataframe(historical_chain_tvl_sheet, eth_tvl_df, include_index=False,\n",
        "                      include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNNELNFhfoxF"
      },
      "source": [
        "### 2.5. Update 'Protocol Info' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all protocols\n",
        "protocols = requests.get(tvl_base_url + '/protocols')\n",
        "\n",
        "# Convert Protocols response to data frame\n",
        "protocols_df = pd.DataFrame(protocols.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear every cell in the sheet\n",
        "protocol_info_sheet.clear()\n",
        "\n",
        "# Update data in 'Pool Yields' sheet\n",
        "gd.set_with_dataframe(protocol_info_sheet, protocols_df, include_index=False,\n",
        "                      include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puJVNFsSNKoL"
      },
      "source": [
        "### 2.6. Update data in 'LP Update Historicals' tab for LP Landscape Update distribution charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "kx5LtWWI1u9U"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "# Aggregate historicals for peer pools, store mean values, and return as dataframe\n",
        "def aggregate_historicals(all_pools_dict, pool_id_list, exclude_list):\n",
        "  # Keep only pools not contained in exclude list\n",
        "  pool_id_list = [id for id in pool_id_list if id not in exclude_list]\n",
        "\n",
        "  # Keep only data for pools contained in pool_id_list\n",
        "  peer_pools = {k: all_pools_dict[k] for k in pool_id_list}\n",
        "\n",
        "  # Separate values (dataframes) from keys (pool id's) before concatenating\n",
        "  peer_pools_dfs = list(peer_pools.values())\n",
        "\n",
        "  # Concatenate all dataframes\n",
        "  peer_pools_df = pd.concat(peer_pools_dfs)\n",
        "\n",
        "  # Group by timestamp and compute the mean for each group\n",
        "  aggregated_df = peer_pools_df.groupby('timestamp').mean().reset_index()\n",
        "\n",
        "  # Order by date in reverse chronological order\n",
        "  aggregated_df = aggregated_df.sort_values(by='timestamp', ascending=False)\n",
        "\n",
        "  return aggregated_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "oLeRq2O7NgFv"
      },
      "outputs": [],
      "source": [
        "# Create dictionary to associate all pool ids to their historical data\n",
        "all_pools_dict = dict(zip(pool_ids, pool_historicals_dfs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "x4xYz7WiNlrT"
      },
      "outputs": [],
      "source": [
        "# Select the 'Stable Comps' tab and read the data into a dataframe\n",
        "stable_comps_sheet = lp_landscape.worksheet(\"Stables Comps\")\n",
        "stable_comps = stable_comps_sheet.get_all_records()\n",
        "stable_comps_df = pd.DataFrame(stable_comps)\n",
        "\n",
        "# Select the 'Volatile Comps' tab and read the data into a dataframe\n",
        "volatile_comps_sheet = lp_landscape.worksheet(\"Volatile Comps\")\n",
        "volatile_comps = volatile_comps_sheet.get_all_records()\n",
        "volatile_comps_df = pd.DataFrame(volatile_comps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "sDm-SeSLOnRi"
      },
      "outputs": [],
      "source": [
        "# Extract pool names from the worksheet as a list for each index group\n",
        "\n",
        "base_stable_comps_df = stable_comps_df[stable_comps_df[\"Chain\"] == \"Base\"]\n",
        "\n",
        "eth_stable_comps_df = stable_comps_df[stable_comps_df[\"Chain\"] == \"Ethereum\"]\n",
        "eth_curve_stable_comps_df = eth_stable_comps_df[eth_stable_comps_df[\"API project name\"] == \"curve-dex\"]\n",
        "eth_convex_stable_comps_df = eth_stable_comps_df[eth_stable_comps_df[\"API project name\"] == \"convex-finance\"]\n",
        "\n",
        "eth_curve_comps_df = volatile_comps_df[volatile_comps_df[\"API project name\"] == \"curve-dex\"]\n",
        "eth_convex_comps_df = volatile_comps_df[volatile_comps_df[\"API project name\"] == \"convex-finance\"]\n",
        "\n",
        "\n",
        "\n",
        "base_stable_comps_pool_ids = base_stable_comps_df[\"API pool id\"].tolist()\n",
        "base_stable_comps_pool_ids = [id for id in base_stable_comps_pool_ids if id]\n",
        "\n",
        "eth_stable_comps_pool_ids = eth_stable_comps_df[\"API pool id\"].tolist()\n",
        "eth_stable_comps_pool_ids = [id for id in eth_stable_comps_pool_ids if id]\n",
        "\n",
        "eth_comps_pool_ids = volatile_comps_df[\"API pool id\"].tolist()\n",
        "eth_comps_pool_ids = [id for id in eth_comps_pool_ids if id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write lists to csv files\n",
        "write_list_to_csv(\"eth_stable_comps_pool_ids.csv\", eth_stable_comps_pool_ids)\n",
        "write_list_to_csv(\"base_stable_comps_pool_ids.csv\", base_stable_comps_pool_ids)\n",
        "write_list_to_csv(\"eth_comps_pool_ids.csv\", eth_comps_pool_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "eth_curve_stable_comps_pool_ids = eth_curve_stable_comps_df[\"API pool id\"].tolist()\n",
        "eth_curve_stable_comps_pool_ids = [id for id in eth_curve_stable_comps_pool_ids if id]\n",
        "eth_convex_stable_comps_pool_ids = eth_convex_stable_comps_df[\"API pool id\"].tolist()\n",
        "eth_convex_stable_comps_pool_ids = [id for id in eth_convex_stable_comps_pool_ids if id]\n",
        "\n",
        "eth_curve_comps_pool_ids = eth_curve_comps_df[\"API pool id\"].tolist()\n",
        "eth_curve_comps_pool_ids = [id for id in eth_curve_comps_pool_ids if id]\n",
        "eth_convex_comps_pool_ids = eth_convex_comps_df[\"API pool id\"].tolist()\n",
        "eth_convex_comps_pool_ids = [id for id in eth_convex_comps_pool_ids if id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Pools to exclude\n",
        "# HYUSD-EUSD 2faacc5b-7e32-46f3-84e2-061aed8f7f21; c8815168-ba35-4e7c-b7b1-a0b33b6c73bc\n",
        "# EUSD-FRAXBP c04005c9-7e34-41a6-91c4-295834ed8ac0; 817329d2-07cb-4cbd-82ac-eb9bc0add450\n",
        "# EUSD-USDC 212375c8-694c-4fa5-8b36-05f50c8e61b2\n",
        "# ETH+ 4e6cd326-72d5-4680-8d2f-3481d50e8bb1 Include naked LSTs in Comps? \n",
        "# ETH+-ETH 5a046093-29fc-4ecb-b90e-daccda151b5b; 74346f6f-c7ee-4506-a204-baf48e13decb\n",
        "# ETH+/pxETH when gauge goes into effect"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {},
      "outputs": [],
      "source": [
        "# List of pool_ids to exclude from indices (RToken pools)\n",
        "exclude_pool_ids = [\"2faacc5b-7e32-46f3-84e2-061aed8f7f21\", \"c8815168-ba35-4e7c-b7b1-a0b33b6c73bc\",\n",
        "                    \"c04005c9-7e34-41a6-91c4-295834ed8ac0\", \"817329d2-07cb-4cbd-82ac-eb9bc0add450\",\n",
        "                    \"212375c8-694c-4fa5-8b36-05f50c8e61b2\", \"74346f6f-c7ee-4506-a204-baf48e13decb\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Aggregate data into an index for each set of pools id's\n",
        "eth_curve_stable_index_df = aggregate_historicals(all_pools_dict, eth_curve_stable_comps_pool_ids, exclude_pool_ids)\n",
        "eth_convex_stable_index_df = aggregate_historicals(all_pools_dict, eth_convex_stable_comps_pool_ids, exclude_pool_ids)\n",
        "base_stable_index_df = aggregate_historicals(all_pools_dict, base_stable_comps_pool_ids, exclude_pool_ids)\n",
        "eth_curve_index_df = aggregate_historicals(all_pools_dict, eth_curve_comps_pool_ids, exclude_pool_ids)\n",
        "eth_convex_index_df = aggregate_historicals(all_pools_dict, eth_convex_comps_pool_ids, exclude_pool_ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Merge dataframes to keep TVL from Curve and APY from Convex for each index group\n",
        "eth_curve_stable_index_df.drop('apy', axis=1, inplace=True)\n",
        "eth_convex_stable_index_df.drop('tvlUsd', axis=1, inplace=True)\n",
        "\n",
        "eth_curve_index_df.drop('apy', axis=1, inplace=True)\n",
        "eth_convex_index_df.drop('tvlUsd', axis=1, inplace=True)\n",
        "\n",
        "\n",
        "eth_stable_index_df = pd.merge(eth_curve_stable_index_df, eth_convex_stable_index_df, on='timestamp', how='inner')\n",
        "eth_index_df = pd.merge(eth_curve_index_df, eth_convex_index_df, on='timestamp', how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create list to print as header of sheet with index names\n",
        "index_header = [\"Mainnet Stables Index\", \"Base Stables Index\", \"Mainnet ETH Index\"]\n",
        "\n",
        "# Concatenate every dataframe into one dataframe\n",
        "indexed_historicals_df = pd.concat([eth_stable_index_df, base_stable_index_df, eth_index_df], axis = 1, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HyDVDxjWaumz",
        "outputId": "694e6fe0-5f6b-4c5f-ac47-ac8f10294d51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Insert 2 blank elements between each title in the list for spreadsheet formatting\n",
        "formatted_index_header = []\n",
        "\n",
        "for title in index_header:\n",
        "  formatted_index_header.append(title)\n",
        "  for i in range(2):\n",
        "    formatted_index_header.append(\"\")\n",
        "formatted_index_header.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "A_giEyhhbPfP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vf/7r30jcsd0190g5bqdf7ryl800000gn/T/ipykernel_82881/667794954.py:8: DeprecationWarning: [Deprecated][in version 6.0.0]: Method signature's arguments 'range_name' and 'values' will change their order. We recommend using named arguments for minimal impact. In addition, the argument 'values' will be mandatory of type: 'List[List]'. (ex) Worksheet.update(values = [[]], range_name=) \n",
            "  indexed_historicals_sheet.update(\"A1\", [formatted_index_header])\n"
          ]
        }
      ],
      "source": [
        "# Select 'Indexed Historicals' tab\n",
        "indexed_historicals_sheet = lp_landscape.worksheet(\"Indexed Historicals\")\n",
        "\n",
        "# Clear every cell in the sheet\n",
        "indexed_historicals_sheet.clear()\n",
        "\n",
        "# Write formatted list as first row of the 'Pool Historicals' sheet for indexing\n",
        "indexed_historicals_sheet.update(\"A1\", [formatted_index_header])\n",
        "\n",
        "# Update data in 'LP Update Historicals' sheet under first row\n",
        "gd.set_with_dataframe(indexed_historicals_sheet, indexed_historicals_df, row=2,\n",
        "                      include_index=False, include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Export Stables and Volatile tabs to csv files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract relevant columns\n",
        "stables_data = stables_sheet.get('A:U')\n",
        "\n",
        "# Convert to a DataFrame\n",
        "stables_df = pd.DataFrame(stables_data[1:], columns=stables_data[0])\n",
        "\n",
        "# Export to CSV\n",
        "stables_df.to_csv('stables.csv', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract relevant columns\n",
        "volatile_data = volatile_sheet.get('A:U')\n",
        "\n",
        "# Convert to a DataFrame\n",
        "volatile_df = pd.DataFrame(volatile_data[1:], columns=volatile_data[0])\n",
        "\n",
        "# Export to CSV\n",
        "volatile_df.to_csv('volatile.csv', index=False)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
