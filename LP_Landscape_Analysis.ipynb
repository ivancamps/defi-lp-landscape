{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkSrPyGN0030"
      },
      "source": [
        "# Automated Updates and Calculations in 'DeFi Landscape LP Opportunities'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uefzj5QS1LGk"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyOTepQJ1hFX"
      },
      "source": [
        "### Install and import all relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "KsWIyhkz6UtD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install gspread\n",
        "!pip install pandas\n",
        "!pip install gspread-dataframe\n",
        "!pip install oauth2client\n",
        "!pip install PyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "T0GACseh6dcV"
      },
      "outputs": [],
      "source": [
        "# Import all relevant libraries\n",
        "import pandas as pd\n",
        "import json\n",
        "from datetime import datetime as dt\n",
        "import csv\n",
        "import math\n",
        "import requests\n",
        "import gspread\n",
        "import gspread_dataframe as gd\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry-Z_lO613tC"
      },
      "source": [
        "### Connect to Google Sheets API and setup Defillama API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1vV2-rvL8JX",
        "outputId": "b1747c9e-88ad-4989-bf1d-bce4d0594a70"
      },
      "outputs": [],
      "source": [
        "# Set up Google Sheets API credentials\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "credentials = ServiceAccountCredentials.from_json_keyfile_name('lp-landscape-analysis-dd6d6479b244.json', scope)\n",
        "gc = gspread.authorize(credentials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "UEcUo3T06ghU"
      },
      "outputs": [],
      "source": [
        "# API TVL base URL\n",
        "tvl_base_url = 'https://api.llama.fi'\n",
        "\n",
        "# API Yields base URL\n",
        "yields_base_url = 'https://yields.llama.fi'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbrFKqGO2Liv"
      },
      "source": [
        "## 2. Update Data in 'DeFi Landscape LP Opportunities'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBIh0TWv245K"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "ZderTHLInaNn"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "# Writes a list of values to a column of the input sheet starting at starting_cell\n",
        "def write_to_column(sheet, starting_cell, values):\n",
        "  sheet.update(starting_cell, [[i] for i in values], value_input_option=\"USER_ENTERED\")\n",
        "  print(\"Values written to sheet successfully\")\n",
        "\n",
        "# Converts a dataframe column from str to date format in-place\n",
        "def df_str_to_date(df, column_name, date_format):\n",
        "  df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='raise')\n",
        "  df[column_name] = df[column_name].dt.date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YLemCeB3B6g"
      },
      "source": [
        "### Open spreadsheet and load relevant tabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "B9qHVp9pPKBN"
      },
      "outputs": [],
      "source": [
        "# Open the Google Sheet we'll be reading and writing to\n",
        "lp_landscape = gc.open('DeFi Landscape LP Opportunities_v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "tkFKixHVjeId"
      },
      "outputs": [],
      "source": [
        "# Select the 'Project Ratings' tab and read the data into a dataframe\n",
        "project_ratings_sheet = lp_landscape.worksheet(\"Project Ratings\")\n",
        "lp_project_ratings = project_ratings_sheet.get_all_records()\n",
        "lp_project_ratings_df = pd.DataFrame(lp_project_ratings)\n",
        "\n",
        "# Select the 'Stables' tab and read the data into a dataframe\n",
        "stables_sheet = lp_landscape.worksheet(\"Stables\")\n",
        "lp_stables = stables_sheet.get_all_records()\n",
        "lp_stables_df = pd.DataFrame(lp_stables)\n",
        "\n",
        "# Select 'Pool Yields' tab\n",
        "pool_yields_sheet = lp_landscape.worksheet(\"Pool Yields\")\n",
        "\n",
        "# Select 'Protocol Historicals' tab\n",
        "protocol_historicals_sheet = lp_landscape.worksheet(\"Protocol Historicals\")\n",
        "\n",
        "# Select 'Pool Historicals' tab\n",
        "pool_historicals_sheet = lp_landscape.worksheet(\"Pool Historicals\")\n",
        "\n",
        "# Select 'Protocol Info' tab\n",
        "protocol_info_sheet = lp_landscape.worksheet(\"Protocol Info\")\n",
        "\n",
        "# Select 'Historical Chain TVL' tab\n",
        "historical_chain_tvl_sheet = lp_landscape.worksheet(\"Historical Chain TVL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uym7Wwq_3GnL"
      },
      "source": [
        "### 2.1. Update data in 'Project Ratings' tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "u6iuvMM3p3fw"
      },
      "outputs": [],
      "source": [
        "# Extract protocol names from the worksheet as a list in the format used by Defillama API (slug)\n",
        "protocol_slugs = lp_project_ratings_df[\"API Protocol Name\"].tolist()\n",
        "\n",
        "protocol_slugs = [protocol for protocol in protocol_slugs if protocol]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write output to CSV file \n",
        "protocols_filename = 'protocol_slugs.csv'\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(protocols_filename, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    \n",
        "    # Writing each item in the list to the file\n",
        "    for slug in protocol_slugs:\n",
        "        writer.writerow([slug])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvSe4aLopETQ"
      },
      "source": [
        "#### 2.1.1. Update current protocol TVLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "XBYJW-jYXYAb"
      },
      "outputs": [],
      "source": [
        "# Get current TVLs for the list of protocols and return them as a list\n",
        "protocol_tvls = [requests.get(tvl_base_url + '/tvl/' + protocol).json() for protocol in protocol_slugs]\n",
        "\n",
        "# Format numbers as millions before writing to spreadsheet\n",
        "protocol_tvls_m = [int(tvl) / 1000000 for tvl in protocol_tvls]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL6khy_lqvU3",
        "outputId": "c9ed1fb9-d746-4c65-9f31-c445b37616a7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vf/7r30jcsd0190g5bqdf7ryl800000gn/T/ipykernel_70812/3993001569.py:5: DeprecationWarning: [Deprecated][in version 6.0.0]: Method signature's arguments 'range_name' and 'values' will change their order. We recommend using named arguments for minimal impact. In addition, the argument 'values' will be mandatory of type: 'List[List]'. (ex) Worksheet.update(values = [[]], range_name=) \n",
            "  sheet.update(starting_cell, [[i] for i in values], value_input_option=\"USER_ENTERED\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Values written to sheet successfully\n"
          ]
        }
      ],
      "source": [
        "# Write results stored in protocol_tvls to 'Current TVL ($m)' in 'Project Ratings' tab to update current protocol TVLs\n",
        "write_to_column(project_ratings_sheet, \"C2\", protocol_tvls_m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_oYs6sfpW1K"
      },
      "source": [
        "#### 2.1.2. Update 1-year average protocol TVLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "fEPRILvz6LZI"
      },
      "outputs": [],
      "source": [
        "# Pull historical TVL data for each protocol in the list broken down by token and chain\n",
        "protocol_historicals = [requests.get(tvl_base_url + '/protocol/' + protocol).json() for protocol in protocol_slugs]\n",
        "\n",
        "# List of keys we want to keep\n",
        "fields = ['name','currentChainTvls']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNOEFuiYpd3A"
      },
      "source": [
        "##### Sanity check to compare aggregated TVL to actual current TVL shown by Defillama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "SaW0zUqBr5D0"
      },
      "outputs": [],
      "source": [
        "# Create a list of dictionaries with protocol name as the key as current TVLs\n",
        "# by chain as the value for every protocol in the list\n",
        "\n",
        "# List to store broken down results\n",
        "protocol_tvls_by_chain = []\n",
        "\n",
        "# List to store aggreagated TVL results\n",
        "aggregated_protocol_tvls = []\n",
        "\n",
        "# Iterate protocol list\n",
        "for protocol in protocol_historicals:\n",
        "  # keep only relevant keys\n",
        "  p = dict((k, protocol[k]) for k in fields if k in protocol)\n",
        "\n",
        "  # Create key,value pair from the values associated with the two keys left\n",
        "  p = {protocol['name']: protocol['currentChainTvls']}\n",
        "\n",
        "  # Iterate nested dict and remove unwanted chains to prevent double counting TVL\n",
        "  for k,v in p.items():\n",
        "    for x in list(v.keys()):\n",
        "      if \"borrowed\" in x or \"staking\" in x or 'pool2' in x:\n",
        "        del v[x]\n",
        "    aggregated = (k, sum(v.values()))\n",
        "\n",
        "  # Add protocol to result list\n",
        "  protocol_tvls_by_chain.append(p)\n",
        "\n",
        "  # Add protocol overall current TVL to aggregated list\n",
        "  aggregated_protocol_tvls.append(aggregated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "Qx8dAXGepcCG"
      },
      "outputs": [],
      "source": [
        "# COMPARE AGGREGATED VS. ACTUAL TVLs\n",
        "\n",
        "current_aggregated_protocol_tvls = [x[1] for x in aggregated_protocol_tvls]\n",
        "protocol_names = [x[0] for x in aggregated_protocol_tvls]\n",
        "\n",
        "# Create tuples for each protocol comparing aggregated TVL to TVL shown by DL\n",
        "protocol_tvl_deltas = list(zip(protocol_names, zip(current_aggregated_protocol_tvls, protocol_tvls)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iunEhYG6xKo",
        "outputId": "70e311fc-455e-46d9-dd6a-4db813ab4648"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[('AAVE', (6169791107.24858, 6169791107.248594)),\n",
              " ('Curve Finance', (1945417529.10718, 1939004175.7370179)),\n",
              " ('Compound', (1150882528.97724, 1145397246.3872075)),\n",
              " ('Convex Finance', (1999411054.82827, 1999411054.828274)),\n",
              " ('Reserve', (27295026.85789, 27295026.85788813)),\n",
              " ('MakerDAO', (5790214660.16232, 5597384507.413509)),\n",
              " ('Spark', (1439376104.36094, 1423021289.9270036)),\n",
              " ('Yearn Finance', (350811777.80176, 350811777.8017609)),\n",
              " ('Frax Finance', (956742663.25313, 956742663.253129)),\n",
              " ('Goldfinch', (1117831.27696, 1117831.2769619313)),\n",
              " ('Balancer', (910754091.4576501, 910754091.4576555)),\n",
              " ('Flux Finance', (29111812.15055, 29111812.150552373)),\n",
              " ('Summer.fi', (2865308489.6061, 2865210937.4579725)),\n",
              " ('Stargate', (322015705.25694, 322015705.2569519)),\n",
              " ('Aura', (387027282.93618, 387027282.9361881)),\n",
              " ('Abracadabra', (144636445.03667998, 144636445.0366732)),\n",
              " ('Velodrome', (126764999.59662, 126764999.59661971)),\n",
              " ('Idle', (32001115.472319998, 32001115.472324044)),\n",
              " ('Gains Network', (31467028.66683, 31467028.666822456)),\n",
              " ('Morpho', (567215414.01483, 567215414.0148255)),\n",
              " ('Notional', (25897821.34374, 25897821.343742274)),\n",
              " ('Extra Finance', (22117636.79217, 22117636.792167023)),\n",
              " ('Tangible', (42015525.82033, 42015525.820329495)),\n",
              " ('IPOR', (13625080.54114, 13625080.54113577)),\n",
              " ('Across', (102727423.72375, 102727423.72374555)),\n",
              " ('Inverse Finance', (81714080.66146, 81714080.66145627)),\n",
              " ('Synthetix', (837546182.69322, 837546182.6932225)),\n",
              " ('UwU Lend', (104945618.60364, 105083425.56574553)),\n",
              " ('Origin Dollar', (14726576.61884, 14726576.618844237))]"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "protocol_tvl_deltas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXxv__mz6VEW"
      },
      "source": [
        "##### Aggregate historical TVL broken down by token and chain to get overall historical protocol TVLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "SADoTi_z6tyj"
      },
      "outputs": [],
      "source": [
        "# Create a list of dictionaries with protocol name as the key and historical TVL\n",
        "# by token and chain as the value for every protocol in the list\n",
        "\n",
        "# List to store broken down results\n",
        "clean_protocol_historicals = []\n",
        "\n",
        "# Iterate protocol list\n",
        "for protocol in protocol_historicals:\n",
        "  # Keep only relevant keys\n",
        "  p = dict((k, protocol[k]) for k in fields if k in protocol)\n",
        "\n",
        "  # Create key,value pair from the values associated with the two keys left\n",
        "  p = {protocol['name']: protocol['chainTvls']}\n",
        "\n",
        "  # Iterate nested dict and remove unwanted chains to prevent double counting TVL\n",
        "  for k,v in p.items():\n",
        "    for x in list(v.keys()):\n",
        "      if \"borrowed\" in x or \"staking\" in x or 'pool2' in x:\n",
        "        del v[x]\n",
        "\n",
        "  # Add protocol to result list\n",
        "  clean_protocol_historicals.append(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "icBD3MLIpJtN"
      },
      "outputs": [],
      "source": [
        "# List to store the final result\n",
        "historical_protocol_tvls = []\n",
        "\n",
        "# Aggregate TVL data per date for each chain\n",
        "for protocol in clean_protocol_historicals:\n",
        "\n",
        "  # Dictionary to store result\n",
        "  aggregated_historicals = defaultdict(float)\n",
        "\n",
        "  for p, d1 in protocol.items():\n",
        "    for chain, chain_data in d1.items():\n",
        "        for entry in chain_data[\"tvl\"]:\n",
        "            date = entry[\"date\"]\n",
        "            totalLiquidityUSD = float(entry[\"totalLiquidityUSD\"])\n",
        "            aggregated_historicals[date] += totalLiquidityUSD\n",
        "\n",
        "  # Convert the aggregated data to a sorted list of tuples\n",
        "  sorted_aggregated_historicals = sorted(aggregated_historicals.items(), key=lambda x: x[0])\n",
        "\n",
        "  # Convert the aggregated data to a dataframe\n",
        "  aggregated_historicals_df = pd.DataFrame(sorted_aggregated_historicals, columns=['date', 'totalLiquidityUSD'])\n",
        "\n",
        "  # Convert the 'Date' column from UNIX timestamp to datetime format\n",
        "  aggregated_historicals_df['date'] = pd.to_datetime(aggregated_historicals_df['date'], unit='s')\n",
        "  aggregated_historicals_df['date'] = aggregated_historicals_df['date'].dt.date\n",
        "\n",
        "  # Add dataframe to result list\n",
        "  historical_protocol_tvls.append(aggregated_historicals_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "AlSLIFZ47LoK"
      },
      "outputs": [],
      "source": [
        "# Order by date in reverse chronological order\n",
        "historical_protocol_tvls = [df.sort_values(by='date', ascending=False) for df in historical_protocol_tvls]\n",
        "\n",
        "# Create a common index for all dataframes to conserve the order when concatenating\n",
        "for df in historical_protocol_tvls:\n",
        "    df.index = range(len(df))\n",
        "\n",
        "# Concatenate every dataframe in the list into one dataframe\n",
        "protocol_historicals_df = pd.concat(historical_protocol_tvls, axis = 1, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "y-Ysu1OQ5m2y"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vf/7r30jcsd0190g5bqdf7ryl800000gn/T/ipykernel_70812/2339619863.py:13: DeprecationWarning: [Deprecated][in version 6.0.0]: Method signature's arguments 'range_name' and 'values' will change their order. We recommend using named arguments for minimal impact. In addition, the argument 'values' will be mandatory of type: 'List[List]'. (ex) Worksheet.update(values = [[]], range_name=) \n",
            "  protocol_historicals_sheet.update(\"A1\", [formatted_protocol_slugs])\n"
          ]
        }
      ],
      "source": [
        "# Insert 1 blank cell between each protocol slug in the list for spreadsheet formatting\n",
        "formatted_protocol_slugs = []\n",
        "\n",
        "for slug in protocol_slugs:\n",
        "  formatted_protocol_slugs.append(slug)\n",
        "  formatted_protocol_slugs.append(\"\")\n",
        "formatted_protocol_slugs.pop()\n",
        "\n",
        "# Clear every cell in the sheet\n",
        "protocol_historicals_sheet.clear()\n",
        "\n",
        "# Write formatted list as first row of the 'Protocol Historicals' sheet for indexing\n",
        "protocol_historicals_sheet.update(\"A1\", [formatted_protocol_slugs])\n",
        "\n",
        "# Update data in 'Protocol Historicals' sheet under first row\n",
        "gd.set_with_dataframe(protocol_historicals_sheet, protocol_historicals_df, row=2,\n",
        "                      include_index=False, include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O5P-vLcu0ow"
      },
      "source": [
        "### 2.2 Update 'Pool Yields' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "k87k0oPTt7FU"
      },
      "outputs": [],
      "source": [
        "# Get all pool yields\n",
        "yields = requests.get(yields_base_url + '/pools')\n",
        "\n",
        "# Convert Yields response to data frame\n",
        "pool_yields_df = pd.DataFrame(yields.json()['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "zAwTLqMuVr6W"
      },
      "outputs": [],
      "source": [
        "# Clear every cell in the sheet\n",
        "pool_yields_sheet.clear()\n",
        "\n",
        "# Update data in 'Pool Yields' sheet\n",
        "gd.set_with_dataframe(pool_yields_sheet, pool_yields_df, include_index=False,\n",
        "                      include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2hBh1Q3sN7"
      },
      "source": [
        "### 2.3 Update 'Pool Historicals' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "9JGcvfoE36Rm"
      },
      "outputs": [],
      "source": [
        "# Extract pool ID's from the worksheet as a list\n",
        "pool_ids = lp_stables_df[\"API pool id\"].tolist()\n",
        "pool_ids = [id for id in pool_ids if id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Write output to CSV file \n",
        "pool_ids_filename = 'pool_ids.csv'\n",
        "\n",
        "# Open the file in write mode\n",
        "with open(pool_ids_filename, mode='w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    \n",
        "    # Writing each item in the list to the file\n",
        "    for id in pool_ids:\n",
        "        writer.writerow([id])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get historical TVL and APY data for each pool and return it as a list of data frames\n",
        "pool_historicals = [requests.get(yields_base_url + '/chart/' + id).json()['data'] for id in pool_ids]\n",
        "pool_historicals_dfs = [pd.DataFrame(i).drop(['il7d', 'apyBase7d'], axis=1) for i in pool_historicals]\n",
        "\n",
        "# Convert timestamp format from str to date in every data frame\n",
        "date_format = '%Y-%m-%dT%H:%M:%S.%fZ'\n",
        "\n",
        "for df in pool_historicals_dfs:\n",
        "  df_str_to_date(df, 'timestamp', date_format)\n",
        "\n",
        "# Order by date in reverse chronological order\n",
        "pool_historicals_dfs = [df.sort_values(by='timestamp', ascending=False) for df in pool_historicals_dfs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a1flnwVrW2Ht",
        "outputId": "c8195a0e-c109-462e-f47c-4fa6c9a5ebb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 92,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a common index for all dataframes to conserve the order when concatenating\n",
        "for df in pool_historicals_dfs:\n",
        "    df.index = range(len(df))\n",
        "\n",
        "# Concatenate every data frame in the list into one data frame\n",
        "pool_historicals_df = pd.concat(pool_historicals_dfs, axis = 1, sort=False)\n",
        "\n",
        "# Insert 4 blank elements between each pool id in the list for spreadsheet formatting\n",
        "formatted_pool_ids = []\n",
        "\n",
        "for id in pool_ids:\n",
        "  formatted_pool_ids.append(id)\n",
        "  for i in range(4):\n",
        "    formatted_pool_ids.append(\"\")\n",
        "formatted_pool_ids.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "x0jbl4PBVqHv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vf/7r30jcsd0190g5bqdf7ryl800000gn/T/ipykernel_70812/4049637518.py:5: DeprecationWarning: [Deprecated][in version 6.0.0]: Method signature's arguments 'range_name' and 'values' will change their order. We recommend using named arguments for minimal impact. In addition, the argument 'values' will be mandatory of type: 'List[List]'. (ex) Worksheet.update(values = [[]], range_name=) \n",
            "  pool_historicals_sheet.update(\"A1\", [formatted_pool_ids])\n"
          ]
        }
      ],
      "source": [
        "# Clear every cell in the sheet\n",
        "pool_historicals_sheet.clear()\n",
        "\n",
        "# Write formatted list as first row of the 'Pool Historicals' sheet for indexing\n",
        "pool_historicals_sheet.update(\"A1\", [formatted_pool_ids])\n",
        "\n",
        "# Update data in 'Pool Historicals' sheet under first row\n",
        "gd.set_with_dataframe(pool_historicals_sheet, pool_historicals_df, row=2,\n",
        "                      include_index=False, include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9w44YUyOdTD"
      },
      "source": [
        "### 2.4. Update 'Historical Chain TVL' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "ggNYIj2yOoTQ"
      },
      "outputs": [],
      "source": [
        "# Get historical TVL data for Ethereum\n",
        "eth_tvl = requests.get(tvl_base_url + '/v2/historicalChainTvl/Ethereum')\n",
        "\n",
        "# Convert response to data frame\n",
        "eth_tvl_df = pd.DataFrame(eth_tvl.json())\n",
        "\n",
        "# Convert the 'Date' column from UNIX timestamp to datetime format\n",
        "eth_tvl_df['date'] = pd.to_datetime(eth_tvl_df['date'], unit='s')\n",
        "eth_tvl_df['date'] = eth_tvl_df['date'].dt.date\n",
        "\n",
        "# Order by date in reverse chronological order\n",
        "eth_tvl_df = eth_tvl_df.sort_values(by='date', ascending=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 108,
      "metadata": {
        "id": "xyTTtSk8PJlV"
      },
      "outputs": [],
      "source": [
        "# Clear every cell in the sheet\n",
        "historical_chain_tvl_sheet.clear()\n",
        "\n",
        "# Update data in 'Historical Chain TVL' sheet\n",
        "gd.set_with_dataframe(historical_chain_tvl_sheet, eth_tvl_df, include_index=False,\n",
        "                      include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNNELNFhfoxF"
      },
      "source": [
        "### 2.5. Update 'Protocol Info' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Get all protocols\n",
        "protocols = requests.get(tvl_base_url + '/protocols')\n",
        "\n",
        "# Convert Protocols response to data frame\n",
        "protocols_df = pd.DataFrame(protocols.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clear every cell in the sheet\n",
        "protocol_info_sheet.clear()\n",
        "\n",
        "# Update data in 'Pool Yields' sheet\n",
        "gd.set_with_dataframe(protocol_info_sheet, protocols_df, include_index=False,\n",
        "                      include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puJVNFsSNKoL"
      },
      "source": [
        "### 2.6. Update data in 'LP Update Historicals' tab for LP Landscape Update distribution charts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {
        "id": "oLeRq2O7NgFv"
      },
      "outputs": [],
      "source": [
        "# Create dictionary to associate all pool ids to their historical data\n",
        "all_pools_dict = dict(zip(pool_ids, pool_historicals_dfs))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 112,
      "metadata": {
        "id": "x4xYz7WiNlrT"
      },
      "outputs": [],
      "source": [
        "# Select the 'eUSD Curve Comps' tab and read the data into a dataframe\n",
        "eusd_curve_sheet = lp_landscape.worksheet(\"eUSD Curve Comps\")\n",
        "lp_eusd_curve = eusd_curve_sheet.get_all_records()\n",
        "lp_eusd_curve_df = pd.DataFrame(lp_eusd_curve)\n",
        "\n",
        "# Select 'LP Update Historicals' tab\n",
        "lp_update_historicals_sheet = lp_landscape.worksheet(\"LP Update Historicals\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 113,
      "metadata": {
        "id": "sDm-SeSLOnRi"
      },
      "outputs": [],
      "source": [
        "# Extract pool names from the worksheet as a list\n",
        "eusd_peer_pool_ids = lp_eusd_curve_df[\"API pool id\"].tolist()\n",
        "eusd_peer_pool_ids = [id for id in eusd_peer_pool_ids if id]\n",
        "\n",
        "\n",
        "# MIM-3CRV pool id\n",
        "mim_3crv_pool_id = \"8a20c472-142c-4442-b724-40f2183c073e\"\n",
        "\n",
        "# Remove MIM-3CRV pool from list if present\n",
        "if mim_3crv_pool_id in eusd_peer_pool_ids: eusd_peer_pool_ids.remove(mim_3crv_pool_id)\n",
        "\n",
        "# Keep only data for pools contained in pool id list\n",
        "eusd_curve_peer_pools = {k: all_pools_dict[k] for k in eusd_peer_pool_ids}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "hYlR11iBO7XC"
      },
      "outputs": [],
      "source": [
        "# Separate values (dataframes) from keys (pool id's) before concatenating\n",
        "eusd_curve_peer_pools_dfs = list(eusd_curve_peer_pools.values())\n",
        "\n",
        "# Concatenate every dataframe in the list into one dataframe\n",
        "eusd_curve_peer_pools_df = pd.concat(eusd_curve_peer_pools_dfs, axis = 1, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HyDVDxjWaumz",
        "outputId": "694e6fe0-5f6b-4c5f-ac47-ac8f10294d51"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "''"
            ]
          },
          "execution_count": 115,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Insert 4 blank elements between each pool id in the list for spreadsheet formatting\n",
        "formatted_peer_pool_ids = []\n",
        "\n",
        "for id in eusd_curve_peer_pools:\n",
        "  formatted_peer_pool_ids.append(id)\n",
        "  for i in range(4):\n",
        "    formatted_peer_pool_ids.append(\"\")\n",
        "formatted_peer_pool_ids.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "id": "A_giEyhhbPfP"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vf/7r30jcsd0190g5bqdf7ryl800000gn/T/ipykernel_70812/1997130594.py:5: DeprecationWarning: [Deprecated][in version 6.0.0]: Method signature's arguments 'range_name' and 'values' will change their order. We recommend using named arguments for minimal impact. In addition, the argument 'values' will be mandatory of type: 'List[List]'. (ex) Worksheet.update(values = [[]], range_name=) \n",
            "  lp_update_historicals_sheet.update(\"A1\", [formatted_peer_pool_ids])\n"
          ]
        }
      ],
      "source": [
        "# Clear every cell in the sheet\n",
        "lp_update_historicals_sheet.clear()\n",
        "\n",
        "# Write formatted list as first row of the 'Pool Historicals' sheet for indexing\n",
        "lp_update_historicals_sheet.update(\"A1\", [formatted_peer_pool_ids])\n",
        "\n",
        "# Update data in 'LP Update Historicals' sheet under first row\n",
        "gd.set_with_dataframe(lp_update_historicals_sheet, eusd_curve_peer_pools_df, row=2,\n",
        "                      include_index=False, include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbMyRhgFLmvN"
      },
      "source": [
        "### 2.7. Aggregate historical data to create indices for RToken peer groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "2XCyvWQrI4d_"
      },
      "outputs": [],
      "source": [
        "# Select the 'hyUSD Comps' tab and read the data into a dataframe\n",
        "hyusd_sheet = lp_landscape.worksheet(\"hyUSD Comps\")\n",
        "lp_hyusd = hyusd_sheet.get_all_records()\n",
        "lp_hyusd_df = pd.DataFrame(lp_hyusd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "id": "_Ho-cM5g1Hij"
      },
      "outputs": [],
      "source": [
        "# RToken pool id's\n",
        "eusd_pool_id = \"381b00d5-b4f8-489c-95cb-40018c72bdd3\"\n",
        "hyusd_pool_id = \"3378bced-4bde-4ccf-b742-7d5c8ebb7720\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "kx5LtWWI1u9U"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "# Aggregate historicals for peer pools, store mean values, and return as dataframe\n",
        "def aggregate_historicals(all_pools_dict, pool_id_list):\n",
        "  # Keep only data for pools contained in pool_id_list\n",
        "  peer_pools = {k: all_pools_dict[k] for k in pool_id_list}\n",
        "\n",
        "  # Separate values (dataframes) from keys (pool id's) before concatenating\n",
        "  peer_pools_dfs = list(peer_pools.values())\n",
        "\n",
        "  # Concatenate all dataframes\n",
        "  peer_pools_df = pd.concat(peer_pools_dfs)\n",
        "\n",
        "  # Group by timestamp and compute the mean for each group\n",
        "  aggregated_df = peer_pools_df.groupby('timestamp').mean().reset_index()\n",
        "\n",
        "  # Order by date in reverse chronological order\n",
        "  aggregated_df = aggregated_df.sort_values(by='timestamp', ascending=False)\n",
        "\n",
        "  return aggregated_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uExurFd53EXn"
      },
      "source": [
        "#### hyUSD Comps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "id": "OrZYSL-iMkz_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/vf/7r30jcsd0190g5bqdf7ryl800000gn/T/ipykernel_70812/1164355104.py:12: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
            "  peer_pools_df = pd.concat(peer_pools_dfs)\n"
          ]
        }
      ],
      "source": [
        "# Extract pool names from the worksheet as a list\n",
        "hyusd_peer_pool_ids = lp_hyusd_df[\"API pool id\"].tolist()\n",
        "hyusd_peer_pool_ids = [id for id in hyusd_peer_pool_ids if id]\n",
        "\n",
        "# Remove hyUSD from list\n",
        "if hyusd_pool_id in hyusd_peer_pool_ids: hyusd_peer_pool_ids.remove(hyusd_pool_id)\n",
        "\n",
        "# Aggregate historicals for peer pools and store mean values as dataframe\n",
        "hyusd_peers_df = aggregate_historicals(all_pools_dict, hyusd_peer_pool_ids)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
