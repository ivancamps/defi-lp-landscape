{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkSrPyGN0030"
      },
      "source": [
        "# Automated Updates and Calculations in 'DeFi Landscape LP Opportunities'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uefzj5QS1LGk"
      },
      "source": [
        "## 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyOTepQJ1hFX"
      },
      "source": [
        "### Install and import all relevant libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KsWIyhkz6UtD"
      },
      "outputs": [],
      "source": [
        "# Install all libraries\n",
        "%%capture\n",
        "%pip install gspread"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T0GACseh6dcV"
      },
      "outputs": [],
      "source": [
        "# Import all relevant libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "from datetime import datetime as dt\n",
        "import time\n",
        "import math\n",
        "import requests\n",
        "import gspread\n",
        "import gspread_dataframe as gd\n",
        "from oauth2client.service_account import ServiceAccountCredentials\n",
        "from google.colab import drive\n",
        "from collections import defaultdict"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ry-Z_lO613tC"
      },
      "source": [
        "### Connect to Google Sheets API and setup Defillama API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z1vV2-rvL8JX",
        "outputId": "b1747c9e-88ad-4989-bf1d-bce4d0594a70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "\n",
        "# Set up Google Sheets API credentials\n",
        "scope = ['https://spreadsheets.google.com/feeds', 'https://www.googleapis.com/auth/drive']\n",
        "credentials = ServiceAccountCredentials.from_json_keyfile_name('/content/drive/My Drive/Colab Notebooks/lp-landscape-analysis-dd6d6479b244.json', scope)\n",
        "gc = gspread.authorize(credentials)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UEcUo3T06ghU"
      },
      "outputs": [],
      "source": [
        "# API TVL base URL\n",
        "tvl_base_url = 'https://api.llama.fi'\n",
        "\n",
        "# API Yields base URL\n",
        "yields_base_url = 'https://yields.llama.fi'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbrFKqGO2Liv"
      },
      "source": [
        "## 2. Update Data in 'DeFi Landscape LP Opportunities'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LBIh0TWv245K"
      },
      "source": [
        "### Helper functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZderTHLInaNn"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "# Writes a list of values to a column of the input sheet starting at starting_cell\n",
        "def write_to_column(sheet, starting_cell, values):\n",
        "  sheet.update(starting_cell, [[i] for i in values], value_input_option=\"USER_ENTERED\")\n",
        "  print(\"Values written to sheet successfully\")\n",
        "\n",
        "# Converts a dataframe column from str to date format in-place\n",
        "def df_str_to_date(df, column_name, date_format):\n",
        "  df[column_name] = pd.to_datetime(df[column_name], format=date_format, errors='raise')\n",
        "  df[column_name] = df[column_name].dt.date"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-YLemCeB3B6g"
      },
      "source": [
        "### Open spreadsheet and load relevant tabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9qHVp9pPKBN"
      },
      "outputs": [],
      "source": [
        "# Open the Google Sheet we'll be reading and writing to\n",
        "lp_landscape = gc.open('DeFi Landscape LP Opportunities_v2')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tkFKixHVjeId"
      },
      "outputs": [],
      "source": [
        "# Select the 'Project Ratings' tab and read the data into a dataframe\n",
        "project_ratings_sheet = lp_landscape.worksheet(\"Project Ratings\")\n",
        "lp_project_ratings = project_ratings_sheet.get_all_records()\n",
        "lp_project_ratings_df = pd.DataFrame(lp_project_ratings)\n",
        "\n",
        "# Select the 'Stables' tab and read the data into a dataframe\n",
        "stables_sheet = lp_landscape.worksheet(\"Stables\")\n",
        "lp_stables = stables_sheet.get_all_records()\n",
        "lp_stables_df = pd.DataFrame(lp_stables)\n",
        "\n",
        "# Select 'Pool Yields' tab\n",
        "pool_yields_sheet = lp_landscape.worksheet(\"Pool Yields\")\n",
        "\n",
        "# Select 'Protocol Historicals' tab\n",
        "protocol_historicals_sheet = lp_landscape.worksheet(\"Protocol Historicals\")\n",
        "\n",
        "# Select 'Pool Historicals' tab\n",
        "pool_historicals_sheet = lp_landscape.worksheet(\"Pool Historicals\")\n",
        "\n",
        "# Select 'Protocol Info' tab\n",
        "protocol_info_sheet = lp_landscape.worksheet(\"Protocol Info\")\n",
        "\n",
        "# Select 'Historical Chain TVL' tab\n",
        "historical_chain_tvl_sheet = lp_landscape.worksheet(\"Historical Chain TVL\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uym7Wwq_3GnL"
      },
      "source": [
        "### 2.1. Update data in 'Project Ratings' tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6iuvMM3p3fw"
      },
      "outputs": [],
      "source": [
        "# Extract protocol names from the worksheet as a list in the format used by Defillama API (slug)\n",
        "protocol_slugs = lp_project_ratings_df[\"API Protocol Name\"].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvSe4aLopETQ"
      },
      "source": [
        "#### 2.1.1. Update current protocol TVLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XBYJW-jYXYAb"
      },
      "outputs": [],
      "source": [
        "# Get current TVLs for the list of protocols and return them as a list\n",
        "protocol_tvls = [requests.get(tvl_base_url + '/tvl/' + protocol).json() for protocol in protocol_slugs]\n",
        "\n",
        "# Format numbers as millions before writing to spreadsheet\n",
        "protocol_tvls_m = [int(tvl) / 1000000 for tvl in protocol_tvls]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mL6khy_lqvU3",
        "outputId": "c9ed1fb9-d746-4c65-9f31-c445b37616a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Values written to sheet successfully\n"
          ]
        }
      ],
      "source": [
        "# Write results stored in protocol_tvls to 'Current TVL ($m)' in 'Project Ratings' tab to update current protocol TVLs\n",
        "write_to_column(project_ratings_sheet, \"C2\", protocol_tvls_m)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_oYs6sfpW1K"
      },
      "source": [
        "#### 2.1.2. Update 1-year average protocol TVLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEPRILvz6LZI"
      },
      "outputs": [],
      "source": [
        "# Pull historical TVL data for each protocol in the list broken down by token and chain\n",
        "protocol_historicals = [requests.get(tvl_base_url + '/protocol/' + protocol).json() for protocol in protocol_slugs]\n",
        "\n",
        "# List of keys we want to keep\n",
        "fields = ['name','currentChainTvls']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNOEFuiYpd3A"
      },
      "source": [
        "##### Sanity check to compare aggregated TVL to actual current TVL shown by Defillama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SaW0zUqBr5D0"
      },
      "outputs": [],
      "source": [
        "# Create a list of dictionaries with protocol name as the key as current TVLs\n",
        "# by chain as the value for every protocol in the list\n",
        "\n",
        "# List to store broken down results\n",
        "protocol_tvls_by_chain = []\n",
        "\n",
        "# List to store aggreagated TVL results\n",
        "aggregated_protocol_tvls = []\n",
        "\n",
        "# Iterate protocol list\n",
        "for protocol in protocol_historicals:\n",
        "  # keep only relevant keys\n",
        "  p = dict((k, protocol[k]) for k in fields if k in protocol)\n",
        "\n",
        "  # Create key,value pair from the values associated with the two keys left\n",
        "  p = {protocol['name']: protocol['currentChainTvls']}\n",
        "\n",
        "  # Iterate nested dict and remove unwanted chains to prevent double counting TVL\n",
        "  for k,v in p.items():\n",
        "    for x in list(v.keys()):\n",
        "      if \"borrowed\" in x or \"staking\" in x or 'pool2' in x:\n",
        "        del v[x]\n",
        "    aggregated = (k, sum(v.values()))\n",
        "\n",
        "  # Add protocol to result list\n",
        "  protocol_tvls_by_chain.append(p)\n",
        "\n",
        "  # Add protocol overall current TVL to aggregated list\n",
        "  aggregated_protocol_tvls.append(aggregated)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qx8dAXGepcCG"
      },
      "outputs": [],
      "source": [
        "# COMPARE AGGREGATED VS. ACTUAL TVLs\n",
        "\n",
        "current_aggregated_protocol_tvls = [x[1] for x in aggregated_protocol_tvls]\n",
        "protocol_names = [x[0] for x in aggregated_protocol_tvls]\n",
        "\n",
        "# Create tuples for each protocol comparing aggregated TVL to TVL shown by DL\n",
        "protocol_tvl_deltas = list(zip(protocol_names, zip(current_aggregated_protocol_tvls, protocol_tvls)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-iunEhYG6xKo",
        "outputId": "70e311fc-455e-46d9-dd6a-4db813ab4648"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('AAVE', (6219762769.32257, 6226875536.167277)),\n",
              " ('Curve Finance', (2075561040.10705, 2077690092.7374172)),\n",
              " ('Compound', (1208497521.15725, 1215164706.9758093)),\n",
              " ('Convex Finance', (1992030725.73304, 1989931697.738274)),\n",
              " ('Reserve', (26945628.83844, 26945628.83843646)),\n",
              " ('MakerDAO', (5842592892.29519, 5842550870.747938)),\n",
              " ('Spark', (1328119064.72071, 1326148184.8194244)),\n",
              " ('Yearn Finance', (351734092.15618, 341321848.5872128)),\n",
              " ('Frax Finance', (946532081.7143799, 946689085.1312767)),\n",
              " ('Goldfinch', (1225722.59509, 1228805.042100636)),\n",
              " ('Balancer', (907462931.0491, 907462931.0490984)),\n",
              " ('Flux Finance', (29948260.16958, 29950121.22272472)),\n",
              " ('Summer.fi', (3146990045.74982, 3146990045.7498236)),\n",
              " ('Stargate', (344621923.37653995, 344621923.37652403)),\n",
              " ('Aura', (404394382.84154, 406470824.5666559)),\n",
              " ('Abracadabra', (132780570.79245, 133059008.21578613)),\n",
              " ('Velodrome', (133243663.42622, 133243663.42622632)),\n",
              " ('Idle', (33054569.31734, 33298613.99447946)),\n",
              " ('Gains Network', (31033002.27022, 31005903.3040728)),\n",
              " ('Morpho', (566844119.42405, 569181454.7113371)),\n",
              " ('Notional', (26376412.75059, 26376412.75058243)),\n",
              " ('Extra Finance', (21383746.59353, 21413881.783482846)),\n",
              " ('Tangible', (42040092.92489, 42040092.92488982)),\n",
              " ('IPOR', (13291568.80227, 13290127.198731367)),\n",
              " ('Across', (103639357.72399, 103883942.457672)),\n",
              " ('Inverse Finance', (77340341.34882, 77340341.34882617)),\n",
              " ('Synthetix', (702580496.35326, 712178587.080482)),\n",
              " ('UwU Lend', (106922545.49088, 106937658.32825069)),\n",
              " ('Origin Dollar', (14723647.54059, 14743797.22383381))]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "protocol_tvl_deltas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXxv__mz6VEW"
      },
      "source": [
        "##### Aggregate historical TVL broken down by token and chain to get overall historical protocol TVLs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SADoTi_z6tyj"
      },
      "outputs": [],
      "source": [
        "# Create a list of dictionaries with protocol name as the key and historical TVL\n",
        "# by token and chain as the value for every protocol in the list\n",
        "\n",
        "# List to store broken down results\n",
        "clean_protocol_historicals = []\n",
        "\n",
        "# Iterate protocol list\n",
        "for protocol in protocol_historicals:\n",
        "  # Keep only relevant keys\n",
        "  p = dict((k, protocol[k]) for k in fields if k in protocol)\n",
        "\n",
        "  # Create key,value pair from the values associated with the two keys left\n",
        "  p = {protocol['name']: protocol['chainTvls']}\n",
        "\n",
        "  # Iterate nested dict and remove unwanted chains to prevent double counting TVL\n",
        "  for k,v in p.items():\n",
        "    for x in list(v.keys()):\n",
        "      if \"borrowed\" in x or \"staking\" in x or 'pool2' in x:\n",
        "        del v[x]\n",
        "\n",
        "  # Add protocol to result list\n",
        "  clean_protocol_historicals.append(p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "icBD3MLIpJtN"
      },
      "outputs": [],
      "source": [
        "# List to store the final result\n",
        "historical_protocol_tvls = []\n",
        "\n",
        "# Aggregate TVL data per date for each chain\n",
        "for protocol in clean_protocol_historicals:\n",
        "\n",
        "  # Dictionary to store result\n",
        "  aggregated_historicals = defaultdict(float)\n",
        "\n",
        "  for p, d1 in protocol.items():\n",
        "    for chain, chain_data in d1.items():\n",
        "        for entry in chain_data[\"tvl\"]:\n",
        "            date = entry[\"date\"]\n",
        "            totalLiquidityUSD = float(entry[\"totalLiquidityUSD\"])\n",
        "            aggregated_historicals[date] += totalLiquidityUSD\n",
        "\n",
        "  # Convert the aggregated data to a sorted list of tuples\n",
        "  sorted_aggregated_historicals = sorted(aggregated_historicals.items(), key=lambda x: x[0])\n",
        "\n",
        "  # Convert the aggregated data to a dataframe\n",
        "  aggregated_historicals_df = pd.DataFrame(sorted_aggregated_historicals, columns=['date', 'totalLiquidityUSD'])\n",
        "\n",
        "  # Convert the 'Date' column from UNIX timestamp to datetime format\n",
        "  aggregated_historicals_df['date'] = pd.to_datetime(aggregated_historicals_df['date'], unit='s')\n",
        "  aggregated_historicals_df['date'] = aggregated_historicals_df['date'].dt.date\n",
        "\n",
        "  # Add dataframe to result list\n",
        "  historical_protocol_tvls.append(aggregated_historicals_df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AlSLIFZ47LoK"
      },
      "outputs": [],
      "source": [
        "# Order by date in reverse chronological order\n",
        "historical_protocol_tvls = [df.sort_values(by='date', ascending=False) for df in historical_protocol_tvls]\n",
        "\n",
        "# Create a common index for all dataframes to conserve the order when concatenating\n",
        "for df in historical_protocol_tvls:\n",
        "    df.index = range(len(df))\n",
        "\n",
        "# Concatenate every dataframe in the list into one dataframe\n",
        "protocol_historicals_df = pd.concat(historical_protocol_tvls, axis = 1, sort=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-Ysu1OQ5m2y"
      },
      "outputs": [],
      "source": [
        "# Insert 1 blank cell between each protocol slug in the list for spreadsheet formatting\n",
        "formatted_protocol_slugs = []\n",
        "\n",
        "for slug in protocol_slugs:\n",
        "  formatted_protocol_slugs.append(slug)\n",
        "  formatted_protocol_slugs.append(\"\")\n",
        "formatted_protocol_slugs.pop()\n",
        "\n",
        "# Clear every cell in the sheet\n",
        "protocol_historicals_sheet.clear()\n",
        "\n",
        "# Write formatted list as first row of the 'Protocol Historicals' sheet for indexing\n",
        "protocol_historicals_sheet.update(\"A1\", [formatted_protocol_slugs])\n",
        "\n",
        "# Update data in 'Protocol Historicals' sheet under first row\n",
        "gd.set_with_dataframe(protocol_historicals_sheet, protocol_historicals_df, row=2,\n",
        "                      include_index=False, include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O5P-vLcu0ow"
      },
      "source": [
        "### 2.2 Update 'Pool Yields' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k87k0oPTt7FU"
      },
      "outputs": [],
      "source": [
        "# Get all pool yields\n",
        "yields = requests.get(yields_base_url + '/pools')\n",
        "\n",
        "# Convert Yields response to data frame\n",
        "pool_yields_df = pd.DataFrame(yields.json()['data'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zAwTLqMuVr6W"
      },
      "outputs": [],
      "source": [
        "# Clear every cell in the sheet\n",
        "pool_yields_sheet.clear()\n",
        "\n",
        "# Update data in 'Pool Yields' sheet\n",
        "gd.set_with_dataframe(pool_yields_sheet, pool_yields_df, include_index=False,\n",
        "                      include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV2hBh1Q3sN7"
      },
      "source": [
        "### 2.3 Update 'Pool Historicals' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9JGcvfoE36Rm"
      },
      "outputs": [],
      "source": [
        "# Extract pool ID's from the worksheet as a list\n",
        "pool_ids = lp_stables_df[\"API pool id\"].tolist()\n",
        "\n",
        "# Get historical TVL and APY data for each pool and return it as a list of data frames\n",
        "pool_historicals = [requests.get(yields_base_url + '/chart/' + id).json()['data'] for id in pool_ids]\n",
        "pool_historicals_dfs = [pd.DataFrame(i).drop(['il7d', 'apyBase7d'], axis=1) for i in pool_historicals]\n",
        "\n",
        "# Convert timestamp format from str to date in every data frame\n",
        "date_format = '%Y-%m-%dT%H:%M:%S.%fZ'\n",
        "\n",
        "for df in pool_historicals_dfs:\n",
        "  df_str_to_date(df, 'timestamp', date_format)\n",
        "\n",
        "# Order by date in reverse chronological order\n",
        "pool_historicals_dfs = [df.sort_values(by='timestamp', ascending=False) for df in pool_historicals_dfs]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a1flnwVrW2Ht",
        "outputId": "c8195a0e-c109-462e-f47c-4fa6c9a5ebb5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Create a common index for all dataframes to conserve the order when concatenating\n",
        "for df in pool_historicals_dfs:\n",
        "    df.index = range(len(df))\n",
        "\n",
        "# Concatenate every data frame in the list into one data frame\n",
        "pool_historicals_df = pd.concat(pool_historicals_dfs, axis = 1, sort=False)\n",
        "\n",
        "# Insert 4 blank elements between each pool id in the list for spreadsheet formatting\n",
        "formatted_pool_ids = []\n",
        "\n",
        "for id in pool_ids:\n",
        "  formatted_pool_ids.append(id)\n",
        "  for i in range(4):\n",
        "    formatted_pool_ids.append(\"\")\n",
        "formatted_pool_ids.pop()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x0jbl4PBVqHv"
      },
      "outputs": [],
      "source": [
        "# Clear every cell in the sheet\n",
        "pool_historicals_sheet.clear()\n",
        "\n",
        "# Write formatted list as first row of the 'Pool Historicals' sheet for indexing\n",
        "pool_historicals_sheet.update(\"A1\", [formatted_pool_ids])\n",
        "\n",
        "# Update data in 'Pool Historicals' sheet under first row\n",
        "gd.set_with_dataframe(pool_historicals_sheet, pool_historicals_df, row=2,\n",
        "                      include_index=False, include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNNELNFhfoxF"
      },
      "source": [
        "### 2.4. Update 'Protocol Info' raw data tab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qX0ilqCfgQfn"
      },
      "outputs": [],
      "source": [
        "# Get all protocols\n",
        "protocols = requests.get(tvl_base_url + '/protocols')\n",
        "\n",
        "# Convert Protocols response to data frame\n",
        "protocols_df = pd.DataFrame(protocols.json())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a38yNlyhgaGj"
      },
      "outputs": [],
      "source": [
        "# Clear every cell in the sheet\n",
        "protocol_info_sheet.clear()\n",
        "\n",
        "# Update data in 'Pool Yields' sheet\n",
        "gd.set_with_dataframe(protocol_info_sheet, protocols_df, include_index=False,\n",
        "                      include_column_header=True, resize=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.5. Update 'Historical Chain TVL' raw data tab"
      ],
      "metadata": {
        "id": "n9w44YUyOdTD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get historical TVL data for Ethereum\n",
        "eth_tvl = requests.get(tvl_base_url + '/v2/historicalChainTvl/Ethereum')\n",
        "\n",
        "# Convert response to data frame\n",
        "eth_tvl_df = pd.DataFrame(eth_tvl.json())\n",
        "\n",
        "# Convert the 'Date' column from UNIX timestamp to datetime format\n",
        "eth_tvl_df['date'] = pd.to_datetime(eth_tvl_df['date'], unit='s')\n",
        "eth_tvl_df['date'] = eth_tvl_df['date'].dt.date\n",
        "\n",
        "# Order by date in reverse chronological order\n",
        "eth_tvl_df = eth_tvl_df.sort_values(by='date', ascending=False)"
      ],
      "metadata": {
        "id": "ggNYIj2yOoTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear every cell in the sheet\n",
        "protocol_info_sheet.clear()\n",
        "\n",
        "# Update data in 'Historical Chain TVL' sheet\n",
        "gd.set_with_dataframe(historical_chain_tvl_sheet, eth_tvl_df, include_index=False,\n",
        "                      include_column_header=True, resize=True)"
      ],
      "metadata": {
        "id": "xyTTtSk8PJlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.6. Update data in 'LP Update Historicals' tab for LP Landscape Update distribution charts"
      ],
      "metadata": {
        "id": "puJVNFsSNKoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create dictionary to associate all pool ids to their historical data\n",
        "all_pools_dict = dict(zip(pool_ids, pool_historicals_dfs))"
      ],
      "metadata": {
        "id": "oLeRq2O7NgFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select the 'eUSD Curve Comps' tab and read the data into a dataframe\n",
        "eusd_curve_sheet = lp_landscape.worksheet(\"eUSD Curve Comps\")\n",
        "lp_eusd_curve = eusd_curve_sheet.get_all_records()\n",
        "lp_eusd_curve_df = pd.DataFrame(lp_eusd_curve)\n",
        "\n",
        "# Select 'LP Update Historicals' tab\n",
        "lp_update_historicals_sheet = lp_landscape.worksheet(\"LP Update Historicals\")"
      ],
      "metadata": {
        "id": "x4xYz7WiNlrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract pool names from the worksheet as a list\n",
        "eusd_peer_pool_ids = lp_eusd_curve_df[\"API pool id\"].tolist()\n",
        "\n",
        "# MIM-3CRV pool id\n",
        "mim_3crv_pool_id = \"8a20c472-142c-4442-b724-40f2183c073e\"\n",
        "\n",
        "# Remove MIM-3CRV pool from list if present\n",
        "if mim_3crv_pool_id in eusd_peer_pool_ids: eusd_peer_pool_ids.remove(mim_3crv_pool_id)\n",
        "\n",
        "# Keep only data for pools contained in pool id list\n",
        "eusd_curve_peer_pools = {k: all_pools_dict[k] for k in eusd_peer_pool_ids}"
      ],
      "metadata": {
        "id": "sDm-SeSLOnRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate values (dataframes) from keys (pool id's) before concatenating\n",
        "eusd_curve_peer_pools_dfs = list(eusd_curve_peer_pools.values())\n",
        "\n",
        "# Concatenate every dataframe in the list into one dataframe\n",
        "eusd_curve_peer_pools_df = pd.concat(eusd_curve_peer_pools_dfs, axis = 1, sort=False)"
      ],
      "metadata": {
        "id": "hYlR11iBO7XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Insert 4 blank elements between each pool id in the list for spreadsheet formatting\n",
        "formatted_peer_pool_ids = []\n",
        "\n",
        "for id in eusd_curve_peer_pools:\n",
        "  formatted_peer_pool_ids.append(id)\n",
        "  for i in range(4):\n",
        "    formatted_peer_pool_ids.append(\"\")\n",
        "formatted_peer_pool_ids.pop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HyDVDxjWaumz",
        "outputId": "694e6fe0-5f6b-4c5f-ac47-ac8f10294d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "''"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear every cell in the sheet\n",
        "lp_update_historicals_sheet.clear()\n",
        "\n",
        "# Write formatted list as first row of the 'Pool Historicals' sheet for indexing\n",
        "lp_update_historicals_sheet.update(\"A1\", [formatted_peer_pool_ids])\n",
        "\n",
        "# Update data in 'LP Update Historicals' sheet under first row\n",
        "gd.set_with_dataframe(lp_update_historicals_sheet, eusd_curve_peer_pools_df, row=2,\n",
        "                      include_index=False, include_column_header=True, resize=True)"
      ],
      "metadata": {
        "id": "A_giEyhhbPfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbMyRhgFLmvN"
      },
      "source": [
        "### 2.7. Aggregate historical data to create indices for RToken peer groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XCyvWQrI4d_"
      },
      "outputs": [],
      "source": [
        "# Select the 'hyUSD Comps' tab and read the data into a dataframe\n",
        "hyusd_sheet = lp_landscape.worksheet(\"hyUSD Comps\")\n",
        "lp_hyusd = hyusd_sheet.get_all_records()\n",
        "lp_hyusd_df = pd.DataFrame(lp_hyusd)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Ho-cM5g1Hij"
      },
      "outputs": [],
      "source": [
        "# RToken pool id's\n",
        "eusd_pool_id = \"381b00d5-b4f8-489c-95cb-40018c72bdd3\"\n",
        "hyusd_pool_id = \"3378bced-4bde-4ccf-b742-7d5c8ebb7720\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kx5LtWWI1u9U"
      },
      "outputs": [],
      "source": [
        "# HELPER FUNCTIONS\n",
        "\n",
        "# Aggregate historicals for peer pools, store mean values, and return as dataframe\n",
        "def aggregate_historicals(all_pools_dict, pool_id_list):\n",
        "  # Keep only data for pools contained in pool_id_list\n",
        "  peer_pools = {k: all_pools_dict[k] for k in pool_id_list}\n",
        "\n",
        "  # Separate values (dataframes) from keys (pool id's) before concatenating\n",
        "  peer_pools_dfs = list(peer_pools.values())\n",
        "\n",
        "  # Concatenate all dataframes\n",
        "  peer_pools_df = pd.concat(peer_pools_dfs)\n",
        "\n",
        "  # Group by timestamp and compute the mean for each group\n",
        "  aggregated_df = peer_pools_df.groupby('timestamp').mean().reset_index()\n",
        "\n",
        "  # Order by date in reverse chronological order\n",
        "  aggregated_df = aggregated_df.sort_values(by='timestamp', ascending=False)\n",
        "\n",
        "  return aggregated_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uExurFd53EXn"
      },
      "source": [
        "#### hyUSD Comps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OrZYSL-iMkz_"
      },
      "outputs": [],
      "source": [
        "# Extract pool names from the worksheet as a list\n",
        "hyusd_peer_pool_ids = lp_hyusd_df[\"API pool id\"].tolist()\n",
        "\n",
        "# Remove hyUSD from list\n",
        "if hyusd_pool_id in hyusd_peer_pool_ids: hyusd_peer_pool_ids.remove(hyusd_pool_id)\n",
        "\n",
        "# Aggregate historicals for peer pools and store mean values as dataframe\n",
        "hyusd_peers_df = aggregate_historicals(all_pools_dict, hyusd_peer_pool_ids)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}